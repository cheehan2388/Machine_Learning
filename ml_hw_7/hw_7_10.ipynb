{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "from libsvm.svmutil import svm_read_problem\n",
    "from tqdm import tqdm  # For progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_stump(X, y, weights):\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    min_error = float('inf')\n",
    "    best_stump = {}\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        feature_values = X[:, feature]\n",
    "        unique_values = np.unique(feature_values)\n",
    "        thresholds = (unique_values[:-1] + unique_values[1:]) / 2  # Midpoints\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            for polarity in [1, -1]:\n",
    "                predictions = polarity * np.sign(X[:, feature] - threshold)\n",
    "                predictions[predictions == 0] = 1  # Handle zero as positive class\n",
    "                \n",
    "                misclassified = predictions != y\n",
    "                weighted_error = np.sum(weights * misclassified)\n",
    "                \n",
    "                if weighted_error < min_error:\n",
    "                    min_error = weighted_error\n",
    "                    best_stump = {\n",
    "                        'feature': feature,\n",
    "                        'threshold': threshold,\n",
    "                        'polarity': polarity\n",
    "                    }\n",
    "                    \n",
    "    return best_stump, min_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_stump(X_train, y_train, X_test, y_test, T=500):\n",
    "    \"\"\"\n",
    "    Implement AdaBoost with decision stumps.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: numpy array of shape (n_samples_train, n_features)\n",
    "    - y_train: numpy array of shape (n_samples_train,)\n",
    "    - X_test: numpy array of shape (n_samples_test, n_features)\n",
    "    - y_test: numpy array of shape (n_samples_test,)\n",
    "    - T: number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    - Ein: list of in-sample errors at each iteration\n",
    "    - Eout: list of out-of-sample errors at each iteration\n",
    "    - epsilons: list of weighted errors at each iteration\n",
    "    \"\"\"\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # Initialize weights\n",
    "    weights = np.ones(n_train) / n_train\n",
    "    \n",
    "    # To store weak learners and their alphas\n",
    "    weak_learners = []\n",
    "    alphas = []\n",
    "    \n",
    "    # To store errors\n",
    "    Ein = []\n",
    "    Eout = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for t in tqdm(range(1, T + 1), desc=\"AdaBoost Iterations\"):\n",
    "        # Find the best decision stump\n",
    "        stump, error = decision_stump(X_train, y_train, weights)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        error = max(error, 1e-10)\n",
    "        \n",
    "        # Compute alpha\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "        \n",
    "        # Store the weak learner and alpha\n",
    "        weak_learners.append(stump)\n",
    "        alphas.append(alpha)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = stump['polarity'] * np.sign(X_train[:, stump['feature']] - stump['threshold'])\n",
    "        predictions[predictions == 0] = 1  # Handle zero as positive class\n",
    "        \n",
    "        # Update weights\n",
    "        weights *= np.exp(-alpha * y_train * predictions)\n",
    "        weights /= np.sum(weights)  # Normalize\n",
    "        \n",
    "        # Compute Ein(gt)\n",
    "        gt_train = np.sign(np.sum([alphas[i] * (weak_learners[i]['polarity'] * np.sign(X_train[:, weak_learners[i]['feature']] - weak_learners[i]['threshold'])) for i in range(t)], axis=0))\n",
    "        gt_train[gt_train == 0] = 1\n",
    "        Ein_t = np.mean(gt_train != y_train)\n",
    "        Ein.append(Ein_t)\n",
    "        \n",
    "        # Compute weighted error epsilon_t\n",
    "        epsilons.append(error)\n",
    "        \n",
    "        # Compute Eout(gt)\n",
    "        gt_test = np.sign(np.sum([alphas[i] * (weak_learners[i]['polarity'] * np.sign(X_test[:, weak_learners[i]['feature']] - weak_learners[i]['threshold'])) for i in range(t)], axis=0))\n",
    "        gt_test[gt_test == 0] = 1\n",
    "        Eout_t = np.mean(gt_test != y_test)\n",
    "        Eout.append(Eout_t)\n",
    "        \n",
    "    return Ein, Eout, epsilons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x):\n",
    "    \"\"\"\n",
    "    Convert list of dictionaries to numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: list of dictionaries\n",
    "    \n",
    "    Returns:\n",
    "    - X: numpy array of shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    n_samples = len(x)\n",
    "    n_features = max([max(sample.keys()) if sample else 0 for sample in x])\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    for i, sample in enumerate(x):\n",
    "        for key, value in sample.items():\n",
    "            X[i, key - 1] = value  # libsvm format indices start at 1\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading testing data...\n",
      "Preprocessing training data...\n",
      "Preprocessing testing data...\n",
      "Running AdaBoost-Stump algorithm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AdaBoost Iterations: 100%|██████████| 500/500 [27:37<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Average Ein(gt): 0.0410\n",
      "Final Average Eout(gt): 0.3933\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load data using libsvm format\n",
    "    print(\"Loading training data...\")\n",
    "    y_train, x_train = svm_read_problem('madelon_train.txt')\n",
    "    print(\"Loading testing data...\")\n",
    "    y_test, x_test = svm_read_problem('madelon_test.txt')\n",
    "    \n",
    "    # Convert labels from {0,1} to {-1,+1} if necessary\n",
    "    y_train = np.array([1 if label > 0 else -1 for label in y_train])\n",
    "    y_test = np.array([1 if label > 0 else -1 for label in y_test])\n",
    "    \n",
    "    # Preprocess data to numpy arrays\n",
    "    print(\"Preprocessing training data...\")\n",
    "    X_train = preprocess_data(x_train)\n",
    "    print(\"Preprocessing testing data...\")\n",
    "    X_test = preprocess_data(x_test)\n",
    "    \n",
    "    # Run AdaBoost-Stump\n",
    "    print(\"Running AdaBoost-Stump algorithm...\")\n",
    "    Ein, Eout, epsilons = adaboost_stump(X_train, y_train, X_test, y_test, T=500)\n",
    "    \n",
    "    # Plotting Ein and epsilon_t\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, 501), Ein, label='Ein(gt)', color='blue')\n",
    "    plt.plot(range(1, 501), epsilons, label='ϵ_t', color='red')\n",
    "    plt.xlabel('Iteration t')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Ein(gt) and ϵ_t over 500 Iterations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('Ein_Epsilon_t_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print final average errors\n",
    "    print(f\"Final Average Ein(gt): {Ein[-1]:.4f}\")\n",
    "    print(f\"Final Average Eout(gt): {Eout[-1]:.4f}\")\n",
    "    \n",
    "    # Optionally, save Ein and Eout for further analysis\n",
    "    np.save('Ein.npy', Ein)\n",
    "    np.save('Eout.npy', Eout)\n",
    "    np.save('Epsilons.npy', epsilons)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
