Machine Learning course offered by profesor Hsuan-Tien Lin .

course url ï¼š "[covered content](https://www.csie.ntu.edu.tw/~htlin/course/ml24fall/)"

.ðŸ¤– NTU Machine Learning: Foundations & SystemsComprehensive Implementation & Theoretical ProofsThis repository contains a series of advanced implementations and theoretical analyses conducted during the Graduate-level Machine Learning course at National Taiwan University (NTU)3333. The curriculum focuses on the mathematical foundations of learning, ranging from Computational Learning Theory to Kernel Methods and Ensemble Learning44444444444444.+4ðŸ›  Core Competencies Demonstrated1. Mathematical Optimization & ProofsMoving beyond "black-box" library usage, this work includes rigorous derivations for:Newton's Method for Logistic Regression: Deriving the Hessian matrix $A_E(w_t) = X^T DX$ to optimize cross-entropy error5555.+1SVM Dual Formulation: Proving the equivalence between Augmented Primal and Dual problems under strong duality666666666.+2Generalization Bounds: Analyzing VC Dimension ($d_{vc}$) and Rademacher complexity to evaluate hypothesis set power7777.+1Kernel Validity: Proving that trigonometric and Gaussian (RBF) functions satisfy Mercerâ€™s condition for valid kernels8888.+12. Algorithmic Implementation (from Scratch)Implementation of foundational and advanced algorithms without relying on high-level fit() wrappers:Perceptron Learning Algorithm (PLA): Variants including cyclic, randomized, and "correcting-until-perfect" on real-world sparse datasets like RCV1999999999.+2Ensemble Methods: Implementation of AdaBoost-Stump and Gradient Boosted Decision Trees (GBDT)10101010.+1Optimization: Stochastic Gradient Descent (SGD) for Multinomial Logistic Regression (MLR) and coordinate descent for Elastic Net regularization11111111.+13. Large-Scale Systems & CompetitionsFinal Project (HTMLB): A binary classification system to predict Major League Baseball game outcomes using Kaggle as a competitive evaluation platform121212121212121212.+4MNIST Digit Classification: Utilizing LIBLINEAR to solve L1-regularized logistic regression problems13131313.+2ðŸ“ˆ Key Technical ProjectsHigh-Dimensional Linear ModelsAnalyzed the behavior of $E_{in}$ vs. $E_{out}$ using the cpusmall dataset14141414.+1Demonstrated Skill: Understanding the Bias-Variance tradeoff through learning curves and polynomial transforms15151515.+1Hard-Margin & Soft-Margin SVMsDerived the optimal margin in Z-space using polynomial transforms $\Phi(x)$ and analyzed the impact of hyperparameters $C$ and $\gamma$ on the number of Support Vectors161616161616161616.+2Key Finding: Lower $\gamma$ values spread the influence of support vectors, leading to smoother decision boundaries and better generalization in noisy datasets17171717.+1Neural Network ArchitectureAnalyzed backpropagation in multi-layer networks and mathematically proved the impossibility of implementing XOR with certain $d-(d-1)-1$ feed-forward architectures181818181818181818.+2ðŸ’» Tech StackLanguages: Python (NumPy, Scipy), C++19191919.+1Libraries: LIBLINEAR, LIBSVM20202020.+1Environments: Gradescope, Kaggle, Discord-based collaboration212121212121212121.+2ðŸ“‘ Course Summary (Fall 2024)Instructor: Prof. Hsuan-Tien Lin (Author of Learning From Data)222222222222222222.+2Level: Advanced Graduate / Honors Undergraduate.Emphasis: Rigorous math, high-performance implementation, and reproducibility23232323.